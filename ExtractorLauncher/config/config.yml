# Template for the config file,
# Defaults to config.yml in ./config.
# The config tool defaults to config/config.config-tool.yml, and uses the same config 
# options, though only EndpointURL and if needed username/password/secure are required.
# The values here are generally the default values, except for the contents of lists etc.
# Your config file can contain only a subset of these config options. Any extra options will
# cause the extractor to fail.

# This is a minimal example configuration for meaningful extraction from OPC-UA to CDF.
# It is recommended to run the config-tool in order to configure the extractor to
# extract all relevant information.

# By default this will extract the OPC-UA node hierarchy to the CDF asset hierarchy
# and stream live data to timeseries. With proper configuration the extractor can
# read string timeseries, events and historical data.

# Version of the config schema
version: 1

source:
    # KEPServer의 OPC-UA 엔드포인트 URL을 실제 주소로 변경
    endpoint-url: "opc.tcp://192.168.11.46:49320"  # KEPServer의 실제 IP와 포트로 변경
    # Publishing interval을 100ms로 설정하여 지연 최소화
    publishing-interval: 1000
    browse-nodes-chunk: 1
    browse-chunk: 1000


extraction:
    # Global prefix for externalId towards pushers. Should be unique to prevent name conflicts in the push destinations.
    # The externalId is: IdPrefix + NamespaceMap[nodeId.NamespaceUri] + nodeId.Identifier
    id-prefix: "kep" # KEPServer용 prefix로 변경

    # Delay in ms between each push of data points to targets
    # Alternatively, use N[timeunit] where timeunit is w, d, h, m, s or ms.
    data-push-delay: 100

    # KEPServer의 실제 태그 구조에 맞게 루트 노드 설정
    root-nodes:
        - namespace-uri: "KEPServerEX"
          node-id: "s=S.A"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.B"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.C"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.D"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.E"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.F"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.G"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.H"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.I"
        #- namespace-uri: "KEPServerEX"
        #  node-id: "s=S.J"
    # - namespace-uri:
        #   node-id:

    # KEPServer의 네임스페이스 매핑
    namespace-map:
        "KEPServerEX": kep
    # Config for how OPC-UA data-types are mapped to destinations
   

    # Time in minutes between each call to browse the OPC-UA directory, then push new nodes to destinations.
    # Note that this is a heavy operation, so this number should not be set too low.
    # Alternatively, use N[timeunit] where timeunit is w, d, h, m, s or ms.
    # You may also use a cron expression on the form "[minute] [hour] [day of month] [month] [day of week]"
    # See https://crontab.cronhub.io/
    auto-rebrowse-period: 0
    # Enable using audit events to discover new nodes. If this is set to true, the client will expect AuditAddNodes/AuditAddReferences
    # events on the server node. These will be used to add new nodes automatically, by recursively browsing from each given ParentId.
    enable-audit-discovery: false

    # Update data in destinations on rebrowse or restart.
    # Set auto-rebrowse-period to some value to do this periodically.
    # Context refers to the structure of the node graph in OPC-UA. (assetId and parentId in CDF)
    # Metadata refers to any information obtained from OPC-UA properties. (metadata in CDF)
    # Enabling anything here will increase the startup- and rebrowse-time of the extractor. Enabling metadata will increase it more.
#    update:
#        objects:
#            name: false
#            description: false
#            context: false
#            metadata: false
#        variables:
#            name: false
#            description: false
#            context: false
#            metadata: false

   
    # A list of transformations to be applied to the source nodes before pushing
    # The possible transformations are
    # "Ignore", ignore the node. This will ignore all descendants of the node.
    # If the filter does not use "is-array", "description" or "parent", this is done
    # while reading, and so children will not be read at all. Otherwise, the filtering happens later.
    # "Property", turn the node into a property, which is treated as metadata.
    # This also applies to descendants. Nested metadata is give a name like "grandparent_parent_variable", for
    # each variable in the tree.
    # "DropSubscriptions", do not subscribe to this node with neither events or data-points.
    # "TimeSeries", do not treat this variable as a property.
    # "AsEvents", convert datapoints generated by this variable to events.
    # "Include", include this node. If any Include filter is present all nodes are ignored by default.
    # "AsEvents", if this matches a variable, treat datapoints generated by this variable as events.
    # There is some overhead associated with the filters. They are applied sequentially, so it can help performance to put
    # "Ignore" filters first. This is also worth noting when it comes to TimeSeries transformations, which can undo Property
    # transformations.
    # It is possible to have multiple of each filter type.
    #
    # name, description, id, namespace, and type-definition can be one of
    #  - A regex string
    #  - A list of strings, in which case the filter is a match if any of these are equal to the value being matched.
    #  - An object on the form
    #    name:
    #      file: ...
    #    where "file" is a path to a local file containing newline-separated values to be matched exactly.
    transformations:
      - type: Ignore
        filter:
          #id: "s=Simulation Examples.Functions._System"
          id: "s=S.A._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.B._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.C._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.D._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.E._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.F._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.G._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.H._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.I._System"
      #- type: Ignore
      #  filter:
      #    id: "s=S.J._System"
      # Type, either "Ignore", "Property", "DropSubscriptions", "TimeSeries", "AsEvents", or "Include"
      # - type:
      #  NodeFilter. All non-null filters must match each node for the transformation to be applied.
      #   filter:
            # Regex on node DisplayName
            # name:
            # Regex on node Description. If this is set, requires description to be non-null.
            # description:
            # Regex on node id. Ids on the form "i=123" or "s=string" are matched.
            # id:
            # Whether the node is an array. If this is set, the filter only matches varables.
            # is-array:
            # Regex on the full namespace of the node id.
            # namespace:
            # Regex on the id of the type definition. On the form "i=123" or "s=string"
            # type-definition:
            # The OPC-UA node class, exact match. Should be one of
            # "Object", "ObjectType", "Variable", "VariableType". The other types will work, but do nothing, since we never read those.
            # node-class:
            # The "historizing" attribute on variables. If this is set, the filter only matches variables.
            # historizing:
            # Another instance of NodeFilter which is applied to the parent node.
            # parent:
    # Configure extractor to trigger a rebrowse when there are changes to specific namespace metadata nodes.
    # Also supports filtering by namespace uris.
#    rebrowse-triggers:
        # A dictionary of nodes to which we would like to listen for changes.
#        targets:
#            namespace-publication-date: false
        # A list of namespace uris from which the targets above will be selected and listen for
#        namespaces:
#        - http://opcfoundation.org/UA/
    # Configure soft deletes. When this is enabled, all read nodes are written to a state store after browse,
    # and nodes that are missing on subsequent browses are marked as deleted from CDF, with a configurable
    # marker.
    # A notable exception is relationships in CDF, which has no metadata, so these are hard-deleted if
    # cognite.delete-relationships is enabled
#    deletes:
        # True to enable deletes. This requires a state store to be configured.
#       enabled: false
        # Name of marker indicating that a node is deleted.
        # Added to metadata, or as a column in Raw.
#        delete-marker: "deleted"

    # Configuration for ingesting status codes to CDF timeseries.
    status-codes:
        # Which data points to ingest to CDF.
        # `All` ingests all datapoints, including bad.
        # `Uncertain` ingests good and uncertain data points.
        # `GoodOnly` ingest only good datapoints.
        status-codes-to-ingest: GoodOnly
        ingest-status-codes: false

subscriptions:
    # Enable subscriptions on data-points.
    data-points: true
    # Enable subscriptions on events. Requires events.enabled to be set to true.
    events: false
    # Modify the DataChangeFilter used for datapoint subscriptions. See OPC-UA reference part 4 7.17.2 for details.
    # These are just passed to the server, they have no effect on extractor behavior.
    # Filters are applied to all nodes, but deadband should only affect some, according to the standard.
    data-change-filter:
        # One of Status, StatusValue, or StatusValueTimestamp.
        trigger: "StatusValueTimestamp"
        # One of None, Absolute, or Percent.
        deadband-type: "None"
        # Double value of the deadband.
        deadband-value: 0
    # Log bad subscription datapoints
    log-bad-values: true
    # Ignore the access level parameter for history and datapoints.
    # This means using the "Historizing" parameter for history, and subscribing to all timeseries, independent of AccessLevel.
    ignore-access-level: false
    # How often the server requests updates from the source system (the source system is often the server itself)
    # 0 uses maximum rate set by server
    # Lower increases server load. This should be set to below the maximum update frequency of the source system.
    sampling-interval: 100
    # Length of internal server queue for each subscribed item. < 2 means that any updates occuring between publish requests are lost.
    queue-length: 1
    # The number of publish requests without a response before the server should send a keep alive message.
    keep-alive-count: 10
    # The number of publish requests without a response before the server should close the subscription.
    # Must be at least 3 * keep-alive-count.
    lifetime-count: 1000
    # Recreate subscriptions that have stopped publishing. Enabled by default.
    recreate-stopped-subscriptions: true
    # Grace period for recreating stopped subscriptions.
    # If this is negative, default to 8 * publishing-interval.
    # Syntax is N[timeunit] where timeunit is w, d, h, m, s or ms.
    recreate-subscription-grace-period: -1
    # List of alternative subscription configurations.
    # The first entry with a matching filter will be used for each node.
    alternative-configs:
        # Filter on node, if this matches or is null, the config will be applied.
        #- filter:
             # Regex match on node external ID.
        #    id:
             # Regex match on node data type, if it is a variable.
        #    data-type:
             # Match on whether this subscription is for data points or events.
        #    is-event-state:
           # See subscriptions.data-change-filter
        #  data-change-filter:
           # See subscriptions.sampling-interval
        #  sampling-interval: 100
           # See subscriptions.queue-length
        #  queue-length: 10
logger:
    console:
        level: "verbose"
    file:
        level: "information"
        path: "logs/log.txt"

influx:
    # Set to true to enable this destination
    enabled: false
    # Host URI, ex localhost:8086
#    host: http://192.168.11.116:58086
    # Influx username
#    username: admin
    # Influx password
#    password: admin
    # Database to connect to, will not be created automatically
#    database: test
    # Replace all instances of NaN or Infinity with this floating point number. If left empty, ignore instead.
#    non-finite-replacement: 0
    # Whether to read start/end-points on startup, where possible. At least one pusher should be able to do this,
    # otherwise back/frontfill will run for the entire history every restart.
#   read-extracted-ranges: true
    # Whether to read start/end-points for events on startup, where possible.
#    read-extracted-event-ranges: false
    # Max number of points to send in each request to influx
#    point-chunk-size: 1000

mqtt: 
    # TCP Broker URL
    #host: rabbitmq313-q
    enabled: true
    host: 192.168.11.116 
    # TCP Broker port
    port: 59183
    # MQTT broker username
    username: admin
    # MQTT broker password
    password: misoinfo1!
    # True to enable TLS
    use-tls: false
    # Allow untrusted server SSL certificates. This is fundamentally unsafe
    allow-untrusted-certificates: true
    # Optional path to a custom certificate authority file for SSL
 #   custom-certificate-authority:
    # Mqtt client id. Should be unique for a given broker.
    client-id: OPC-UA-Ext-Plain-JSON
    # Data set to use for new objects. Existing objects will not be updated
 #   data-set-id: 1234567890123456
    # Assets topic
    asset-topic: opcua/assets
    # Timeseries topic
    ts-topic: opcua/timeseries
    # Events topic
 #   event-topic: opcua/events
    # Datapoints topic
    datapoint-topic: opcua/datapoints
    # Raw topic
    raw-topic: opcua/raw
    # Whether to use gRPC/Protobuf serialization for datapoints
    # If true, datapoints will be serialized using Protobuf (gRPC format) - default and topic will be opcua/datapoints
    # If false, datapoints will be serialized using JSON format and topic will be opcua/datapoints/json
    use-grpc: false
    # Timestamp format for JSON serialization
    # "epoch" - Unix timestamp in milliseconds since epoch (default)
    # "iso8601" - ISO 8601 format (YYYY-MM-DDTHH:mm:ss.SSS+09:00)
    timestamp-format: iso8601
    # Timezone offset for ISO8601 timestamp format
    # Format: "+09:00" for Korea time, "+00:00" for UTC, etc.
    # Only used when timestamp-format is "iso8601"
    timezone-offset: "+09:00"
    # JSON format type for MQTT output
    # Available options:
    # "Legacy" - Original timeseries format (existing format for backward compatibility)
    # "PollingSnapshotObject" - Structured object format for polling/batch reads
    #   - Includes metadata with data_ingest_type, message_timestamp, etc.
    #   - Data section with shared timestamp and tags array
    #   - Optimized for bulk data transmission with minimal size
    # "PollingSnapshotPlain" - Flat structure format for polling/batch reads  
    #   - Includes metadata with camelCase naming (dataIngestType, messageTimestamp)
    #   - Tag values directly in root object with shared timestamp
    #   - Most compact format for high-volume data
    # "Subscription" - Format for subscription-based data with individual timestamps
    #   - Supports multiple data points per tag with different timestamps
    #   - Each tag has its own data array with timestamp, value, sc, dt
    #   - Ideal for event-driven subscription scenarios
    
    json-format-type: Subscription
    
    # Whether to include metadata object in JSON output
    include-metadata: true
    # Whether to include msgRecvStartTimestamp and msgRecvEndTimestamp in metadata
    # Only applicable when include-metadata is true
    include-message-timestamps: true
    # Whether to include data type (dt) field for each tag
    include-data-type: false
    # Whether to include status code (sc) field for each tag
    include-status-code: false
    
    # Adaptive chunking configuration for MQTT message optimization
    # Maximum MQTT message size in bytes (default: 1048576 = 1MB)
    max-message-size: 10485760
    # Maximum number of data points in a single chunk (default: 10000)
    max-chunk-size: 2000
    # Minimum number of data points in a single chunk (default: 10)
    min-chunk-size: 10
    
    # Set to enable storing a list of created assets/timeseries to local litedb.
    # Requires the StateStorage.Location property to be set.
    # If this is left empty, metadata will have to be read each time the extractor restarts.
    # Default is empty
 #   local-state:
    # Timestamp in ms since epoch to invalidate stored mqtt states.
    # On extractor restart, assets/timeseries created before this will be attempted re-created in CDF.
    # They will not be deleted or updated.
 #   invalidate-before: 0
    # Replace all instances of NaN, Infinity or values greater than 1E100 with this floating point number. If left empty, ignore instead.
 #   non-finite-replacement:
    # Do not push any metadata at all. If this is true, plain timeseries without metadata will be created,
    # similarly to raw-metadata, and datapoints will be pushed. Nothing will be written to raw, and no assets will be created.
    # Events will be created, but without asset context.
 #   skip-metadata: false
    # Default empty. Store assets and/or timeseries data in raw. Assets will not be created at all,
    # timeseries will be created with just externalId, isStep and isString.
    # Both timeseries and assets will be persisted in their entirety to raw.
    # Datapoints are not affected, events will be created, but without asset context. The externalId
    # of the source node is added to metadata if applicable.
    # Use different table names for assets and timeseries.
 #   raw-metadata:
        # Database to store data in, required.
 #       database:
        # Table to store assets in.
 #       assets-table:
        # Table to store timeseries in.
 #       timeseries-table:
        # Table to store relationships in
 #       relationships-table:
    # Map metadata to asset/timeseries attributes. Each of "assets" and "timeseries" is a map from property DisplayName to
    # CDF attribute. Legal attributes are "name, description, parentId" and "unit" for timeseries. "parentId" must somehow refer to
    # an existing asset. For timeseries it must be a mapped asset, for assets it can be any asset.
    # Example usage:
    # timeseries:
    #    "EngineeringUnits": "unit"
    #    "EURange": "description"
    # assets:
    #    "Name": "name"
#    metadata-mapping:
#        assets:
#        timeseries:

    # If relationships are eneabled, and written to clean, and deletes are enabled. This needs to be set to
    # true in order to hard delete the relationships.
    #delete-relationships: false